{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadgh76/Federated-Learning/blob/main/My_First_Federated_Steps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0ME1jzekiNxk",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd4ac85-5e77-454d-cddb-254c342862f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num users: 6040\n",
            "Num movies: 3706\n",
            "OrderedDict([('x', <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
            "array([[1907],\n",
            "       [2891],\n",
            "       [1574],\n",
            "       [2785],\n",
            "       [2775]])>), ('y', <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
            "array([[3.],\n",
            "       [3.],\n",
            "       [3.],\n",
            "       [4.],\n",
            "       [3.]], dtype=float32)>)])\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import collections\n",
        "import functools\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def download_movielens_data(dataset_path):\n",
        "  \"\"\"Downloads and copies MovieLens data to local /tmp directory.\"\"\"\n",
        "  if dataset_path.startswith('http'):\n",
        "    r = requests.get(dataset_path)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(path='/tmp')\n",
        "  else:\n",
        "    tf.io.gfile.makedirs('/tmp/ml-1m/')\n",
        "    for filename in ['ratings.dat', 'movies.dat', 'users.dat']:\n",
        "      tf.io.gfile.copy(\n",
        "          os.path.join(dataset_path, filename),\n",
        "          os.path.join('/tmp/ml-1m/', filename),\n",
        "          overwrite=True)\n",
        "\n",
        "download_movielens_data('http://files.grouplens.org/datasets/movielens/ml-1m.zip')\n",
        "\n",
        "def load_movielens_data(\n",
        "    data_directory: str = \"/tmp\",\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  \"\"\"Loads pandas DataFrames for ratings, movies, users from data directory.\"\"\"\n",
        "  # Load pandas DataFrames from data directory. Assuming data is formatted as\n",
        "  # specified in http://files.grouplens.org/datasets/movielens/ml-1m-README.txt.\n",
        "  ratings_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"ratings.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine=\"python\")\n",
        "  movies_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"movies.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=[\"MovieID\", \"Title\", \"Genres\"], engine=\"python\",\n",
        "      encoding = \"ISO-8859-1\")\n",
        "\n",
        "  # Create dictionaries mapping from old IDs to new (remapped) IDs for both\n",
        "  # MovieID and UserID. Use the movies and users present in ratings_df to\n",
        "  # determine the mapping, since movies and users without ratings are unneeded.\n",
        "  movie_mapping = {\n",
        "      old_movie: new_movie for new_movie, old_movie in enumerate(\n",
        "          ratings_df.MovieID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "  user_mapping = {\n",
        "      old_user: new_user for new_user, old_user in enumerate(\n",
        "          ratings_df.UserID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "\n",
        "  # Map each DataFrame consistently using the now-fixed mapping.\n",
        "  ratings_df.MovieID = ratings_df.MovieID.map(movie_mapping)\n",
        "  ratings_df.UserID = ratings_df.UserID.map(user_mapping)\n",
        "  movies_df.MovieID = movies_df.MovieID.map(movie_mapping)\n",
        "\n",
        "  # Remove nulls resulting from some movies being in movies_df but not\n",
        "  # ratings_df.\n",
        "  movies_df = movies_df[pd.notnull(movies_df.MovieID)]\n",
        "\n",
        "  return ratings_df, movies_df\n",
        "\n",
        "ratings_df, movies_df = load_movielens_data()\n",
        "ratings_df.head()\n",
        "movies_df.head()\n",
        "\n",
        "print('Num users:', len(set(ratings_df.UserID)))\n",
        "print('Num movies:', len(set(ratings_df.MovieID)))\n",
        "\n",
        "def create_tf_datasets(ratings_df: pd.DataFrame,\n",
        "                       batch_size: int = 1,\n",
        "                       max_examples_per_user: Optional[int] = None,\n",
        "                       max_clients: Optional[int] = None) -> List[tf.data.Dataset]:\n",
        "  \"\"\"Creates TF Datasets containing the movies and ratings for all users.\"\"\"\n",
        "  num_users = len(set(ratings_df.UserID))\n",
        "  # Optionally limit to `max_clients` to speed up data loading.\n",
        "  if max_clients is not None:\n",
        "    num_users = min(num_users, max_clients)\n",
        "\n",
        "  def rating_batch_map_fn(rating_batch):\n",
        "    \"\"\"Maps a rating batch to an OrderedDict with tensor values.\"\"\"\n",
        "    # Each example looks like: {x: movie_id, y: rating}.\n",
        "    # We won't need the UserID since each client will only look at their own\n",
        "    # data.\n",
        "    return collections.OrderedDict([\n",
        "        (\"x\", tf.cast(rating_batch[:, 1:2], tf.int64)),\n",
        "        (\"y\", tf.cast(rating_batch[:, 2:3], tf.float32))\n",
        "    ])\n",
        "\n",
        "  tf_datasets = []\n",
        "  for user_id in range(num_users):\n",
        "    # Get subset of ratings_df belonging to a particular user.\n",
        "    user_ratings_df = ratings_df[ratings_df.UserID == user_id]\n",
        "\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(user_ratings_df)\n",
        "\n",
        "    # Define preprocessing operations.\n",
        "    tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
        "        buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
        "        rating_batch_map_fn,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    tf_datasets.append(tf_dataset)\n",
        "\n",
        "  return tf_datasets\n",
        "\n",
        "\n",
        "def split_tf_datasets(\n",
        "    tf_datasets: List[tf.data.Dataset],\n",
        "    train_fraction: float = 0.8,\n",
        "    val_fraction: float = 0.1,\n",
        ") -> Tuple[List[tf.data.Dataset], List[tf.data.Dataset], List[tf.data.Dataset]]:\n",
        "  \"\"\"Splits a list of user TF datasets into train/val/test by user.\n",
        "  \"\"\"\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(tf_datasets)\n",
        "\n",
        "  train_idx = int(len(tf_datasets) * train_fraction)\n",
        "  val_idx = int(len(tf_datasets) * (train_fraction + val_fraction))\n",
        "\n",
        "  # Note that the val and test data contains completely different users, not\n",
        "  # just unseen ratings from train users.\n",
        "  return (tf_datasets[:train_idx], tf_datasets[train_idx:val_idx],\n",
        "          tf_datasets[val_idx:])\n",
        "\n",
        "# We limit the number of clients to speed up dataset creation. Feel free to pass\n",
        "# max_clients=None to load all clients' data.\n",
        "tf_datasets = create_tf_datasets(\n",
        "    ratings_df=ratings_df,\n",
        "    batch_size=5,\n",
        "    max_examples_per_user=300,\n",
        "    max_clients=2000)\n",
        "\n",
        "# Split the ratings into training/val/test by client.\n",
        "tf_train_datasets, tf_val_datasets, tf_test_datasets = split_tf_datasets(\n",
        "    tf_datasets,\n",
        "    train_fraction=0.8,\n",
        "    val_fraction=0.1)\n",
        "\n",
        "print(next(iter(tf_train_datasets[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "2mIkIdi9DqJh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfeSaRLRFo7RNGmQp/TmbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}