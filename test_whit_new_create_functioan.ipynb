{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOg/bq8X5rPG3r4WXR8IaOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadgh76/Federated-Learning/blob/main/test_whit_new_create_functioan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "L4o7QUIy8Jz4",
        "outputId": "48b28436-da63-4ab6-8093-0a3c9e0a9166"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b17b2e0d67ac>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_federated\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_federated'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "import types\n",
        "from numpy.lib.arraysetops import unique\n",
        "import collections\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "trn = pd.read_csv(\"/content/kdd_train.csv\")\n",
        "trn.isnull().sum()\n",
        "trn.nunique()\n",
        "types = trn[\"labels\"].unique()\n",
        "types = types[1:]\n",
        "trn[\"labels\"].replace(to_replace=types,value=\"attacking\",inplace = True)\n",
        "c = len(trn.select_dtypes(include=[\"number\"]).columns)\n",
        "\n",
        "cols = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
        "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
        "       'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "       'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
        "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
        "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
        "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
        "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "       'dst_host_srv_rerror_rate']\n",
        "\n",
        "for col in trn.select_dtypes(include=[\"object\"]):\n",
        "    print(col)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols = ['protocol_type','service','flag','labels']\n",
        "for i in cols:\n",
        "    en = LabelEncoder()\n",
        "    trn[i] = en.fit_transform(trn[i])\n",
        "\n",
        "\n",
        "corr_matrix = trn.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "trn.drop(to_drop, axis=1, inplace=True)\n",
        "\n",
        "trn.head(10)\n",
        "X_trn = trn.drop(['labels'] , axis = 1).values\n",
        "Y_trn = trn['labels'].values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_trn = scaler.fit_transform(X_trn)\n",
        "\n",
        "\n",
        "tff.federated_computation(lambda: \"dfkjsdjfjklfsd\")()\n",
        "\n",
        "# trn = tf.convert_to_tensor(trn)\n",
        "# trn = tf.data.Dataset.from_tensors(trn)\n",
        "# train_sample = tf.convert_to_tensor(X_trn)\n",
        "# test_sample =  tf.convert_to_tensor(Y_trn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_datasets(trn: pd.DataFrame,\n",
        "                       batch_size: int = 1,\n",
        "                       max_examples_per_user: Optional[int] = None,\n",
        "                       max_clients: Optional[int] = None) -> List[tf.data.Dataset]:\n",
        "\n",
        "\n",
        "  # num_users = len(set(trn.duration))\n",
        "  # if max_clients is not None:\n",
        "  #   num_users = min(num_users, max_clients)\n",
        "\n",
        "  def rating_batch_map_fn(trn_batch):\n",
        "    return collections.OrderedDict([\n",
        "        (\"x\", tf.cast(trn_batch[:, 0:35], tf.float32)),\n",
        "        (\"y\", tf.cast(trn_batch[:, 35:36], tf.int32))\n",
        "    ])\n",
        "\n",
        "  tf_datasets = []\n",
        "  for user_id in range(5):\n",
        "\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(trn)\n",
        "\n",
        "    # # Define preprocessing operations.\n",
        "    # tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
        "    #     buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
        "    #     rating_batch_map_fn,\n",
        "    #     num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # tf_datasets.append(tf_dataset)\n",
        "    NUM_CLIENTS = 10\n",
        "    NUM_EPOCHS = 5\n",
        "    BATCH_SIZE = 20\n",
        "    SHUFFLE_BUFFER = 100\n",
        "    PREFETCH_BUFFER = 10\n",
        "\n",
        "    tf_dataset= tf_dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
        "          BATCH_SIZE).map(rating_batch_map_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "  return tf_datasets\n",
        "\n",
        "\n",
        "def split_tf_datasets(\n",
        "    tf_datasets: List[tf.data.Dataset],\n",
        "    train_fraction: float = 0.8,\n",
        "    val_fraction: float = 0.1,\n",
        ") -> Tuple[List[tf.data.Dataset], List[tf.data.Dataset], List[tf.data.Dataset]]:\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(tf_datasets)\n",
        "\n",
        "  train_idx = int(len(tf_datasets) * train_fraction)\n",
        "  val_idx = int(len(tf_datasets) * (train_fraction + val_fraction))\n",
        "  return (tf_datasets[:train_idx], tf_datasets[train_idx:val_idx],\n",
        "          tf_datasets[val_idx:])"
      ],
      "metadata": {
        "id": "4J9QNsvH8Vba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_datasets = create_tf_datasets(\n",
        "    trn=trn,\n",
        "    batch_size=50,\n",
        "    max_examples_per_user=30,\n",
        "    max_clients=5)\n",
        "\n",
        "tf_train_datasets, tf_val_datasets, tf_test_datasets = split_tf_datasets(\n",
        "    tf_datasets,\n",
        "    train_fraction=0.7,\n",
        "    val_fraction=0.1)"
      ],
      "metadata": {
        "id": "oyePbzMv8X2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(iter(tf_train_datasets[1])))"
      ],
      "metadata": {
        "id": "AAEz34Wq8Z6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MnistVariables = collections.namedtuple(\n",
        "    'MnistVariables', 'weights bias num_examples loss_sum accuracy_sum')"
      ],
      "metadata": {
        "id": "JHWxY_Ab8efV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mnist_variables():\n",
        "  return MnistVariables(\n",
        "      weights=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(35, 1)),\n",
        "          name='weights',\n",
        "          trainable=True),\n",
        "      bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(2)),\n",
        "          name='bias',\n",
        "          trainable=True),\n",
        "      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),\n",
        "      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),\n",
        "      accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False))"
      ],
      "metadata": {
        "id": "CZSYs1F28fZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_batch(variables, x):\n",
        "  return tf.nn.softmax(tf.matmul(x, variables.weights) + variables.bias)\n",
        "\n",
        "def mnist_forward_pass(variables, batch):\n",
        "  y = predict_on_batch(variables, batch['x'])\n",
        "  predictions = tf.cast(tf.argmax(y, 1), tf.int32)\n",
        "\n",
        "  flat_labels = tf.reshape(batch['y'], [-1])\n",
        "  loss = -tf.reduce_mean(\n",
        "      tf.reduce_sum(tf.one_hot(flat_labels, 2) * tf.math.log(y), axis=[1]))\n",
        "  accuracy = tf.reduce_mean(\n",
        "      tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
        "\n",
        "  num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
        "\n",
        "  variables.num_examples.assign_add(num_examples)\n",
        "  variables.loss_sum.assign_add(loss * num_examples)\n",
        "  variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
        "\n",
        "  return loss, predictions"
      ],
      "metadata": {
        "id": "letjgOMh8fXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_local_unfinalized_metrics(variables):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=[variables.num_examples],\n",
        "      loss=[variables.loss_sum, variables.num_examples],\n",
        "      accuracy=[variables.accuracy_sum, variables.num_examples])"
      ],
      "metadata": {
        "id": "XhcYzimu8ed5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metric_finalizers():\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=tf.function(func=lambda x: x[0]),\n",
        "      loss=tf.function(func=lambda x: x[0] / x[1]),\n",
        "      accuracy=tf.function(func=lambda x: x[0] / x[1]))"
      ],
      "metadata": {
        "id": "eHunaPnE8m5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from collections.abc import Callable\n",
        "\n",
        "class MnistModel(tff.learning.models.VariableModel):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._variables = create_mnist_variables()\n",
        "\n",
        "  @property\n",
        "  def trainable_variables(self):\n",
        "    return [self._variables.weights, self._variables.bias]\n",
        "\n",
        "  @property\n",
        "  def non_trainable_variables(self):\n",
        "    return []\n",
        "\n",
        "  @property\n",
        "  def local_variables(self):\n",
        "    return [\n",
        "        self._variables.num_examples, self._variables.loss_sum,\n",
        "        self._variables.accuracy_sum\n",
        "    ]\n",
        "\n",
        "  @property\n",
        "  def input_spec(self):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.TensorSpec([None, 35], tf.float32),\n",
        "        y=tf.TensorSpec([None, 1], tf.int32))\n",
        "\n",
        "  @tf.function\n",
        "  def predict_on_batch(self, x, training=True):\n",
        "    del training\n",
        "    return predict_on_batch(self._variables, x)\n",
        "\n",
        "  @tf.function\n",
        "  def forward_pass(self, batch, training=True):\n",
        "    del training\n",
        "    loss, predictions = mnist_forward_pass(self._variables, batch)\n",
        "    num_exmaples = tf.shape(batch['x'])[0]\n",
        "    return tff.learning.models.BatchOutput(\n",
        "        loss=loss, predictions=predictions, num_examples=num_exmaples)\n",
        "\n",
        "  @tf.function\n",
        "  def report_local_unfinalized_metrics(\n",
        "      self) -> collections.OrderedDict[str, list[tf.Tensor]]:\n",
        "    \"\"\"Creates an `OrderedDict` of metric names to unfinalized values.\"\"\"\n",
        "    return get_local_unfinalized_metrics(self._variables)\n",
        "\n",
        "  def metric_finalizers(\n",
        "      self) -> collections.OrderedDict[str, Callable[[list[tf.Tensor]], tf.Tensor]]:\n",
        "    \"\"\"Creates an `OrderedDict` of metric names to finalizers.\"\"\"\n",
        "    return get_metric_finalizers()\n",
        "\n",
        "  @tf.function\n",
        "  def reset_metrics(self):\n",
        "    \"\"\"Resets metrics variables to initial value.\"\"\"\n",
        "    for var in self.local_variables:\n",
        "      var.assign(tf.zeros_like(var))"
      ],
      "metadata": {
        "id": "70XsXigY8nbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    MnistModel,\n",
        "    client_optimizer_fn=lambda:tf.keras.optimizers.SGD(learning_rate=0.02))"
      ],
      "metadata": {
        "id": "UQpOwucw8r_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_state = training_process.initialize()"
      ],
      "metadata": {
        "id": "pNyx2KTC8uuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for round_num in range(2, 11):\n",
        "  result = training_process.next(train_state, tf_train_datasets)\n",
        "  train_state = result.state\n",
        "  metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "metadata": {
        "id": "Do0qSVFx8yw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}